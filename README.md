# Mobility and Navigation Aid for the Visually Impaired using TensorFlow lite

![San Jose State University](https://i.imgur.com/cShW5MA.gif?1)
![..](https://i.imgur.com/QIGOoLy.png?1)

A technology driven project developed towards the completion of requirements for CS256

An android application which uses a CNN trained in tensorflow lite to detect obstacles in images captured via the phone's camera and intimates the user about the presence of the obstacle by providing auditory feedback using text-to-speech conversion. 

Steps to Execute the project:
  1. Unzip the project zip file
  2. Open Android Studio 
  3. Select Open existing android project
  4. Browse to the extracted project zip and import it
  5. Once Imported, android studio will ask to install missing components(correct SDK and build tool),  click on install
  6. Click finish after the installation is done, depending on your machine configuration, it will ask for other tools to install, let it install.
  7. Connect an android device in debugging mode to the computer and Run the project
  8. Select you device when asked for which device to run on


Demo for the application can be viewed<a href = "https://drive.google.com/file/d/1Vk5wkUHiQKJ-D9GjSbJgncO0VR-eOMpj/view?usp=sharing" target="_blank"> here</a>

<video controls>
  <source src="https://github.com/shantanuspark/tensorflowMobilityAid/blob/master/demo%20video.mp4" type="video/mp4">
</video>
