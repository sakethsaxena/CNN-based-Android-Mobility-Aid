# Mobility and Navigation Aid for the Visually Impaired using TensorFlow lite

![San Jose State University](https://i.imgur.com/cShW5MA.gif?1)
![..](https://i.imgur.com/QIGOoLy.png?1)

A technology driven collaborative project developed towards the completion of requirements for CS256

An android application which uses a CNN trained in tensorflow lite to detect obstacles in images captured via the phone's camera and intimates the user about the presence of the obstacle by providing auditory feedback using text-to-speech conversion. 

The dataset used for training the CNN was self curated, the dataset consisted of primarily five obstacles which are found most commonly during pedestrian commuting in San Jose based on a self conducted survey - people/pedestriands, bicycles, trash cans, newspaper vending machines and benches. 

Steps to Execute the project:
  1. Unzip the project zip file
  2. Open Android Studio 
  3. Select Open existing android project
  4. Browse to the extracted project zip and import it
  5. Once Imported, android studio will ask to install missing components(correct SDK and build tool),  click on install
  6. Click finish after the installation is done, depending on your machine configuration, it will ask for other tools to install, let it install.
  7. Connect an android device in debugging mode to the computer and Run the project
  8. Select you device when asked for which device to run on
  
You can watch the demo below

[![Watch the Demo](https://i.ytimg.com/vi/4q79qvWoYi8/2.jpg?time=1520637169246)](https://youtu.be/4q79qvWoYi8 "Watch the Demo!")



The project was developed by: 
  Saketh Saxena,
  Akhilesh Jichkar,
  Shantanu Deshmukh. 
